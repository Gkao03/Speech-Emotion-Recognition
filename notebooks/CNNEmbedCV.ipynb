{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJkp8FiNBW2D",
        "outputId": "f0acb506-6b07-4c82-b982-413a00835bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5pGdNEtDO1j",
        "outputId": "0ed59ca0-4037-4e88-d6c8-b5bf8c2982db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allosaurus in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allosaurus) (1.19.5)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allosaurus) (0.5.3)\n",
            "Requirement already satisfied: panphon in /usr/local/lib/python3.7/dist-packages (from allosaurus) (0.19)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from allosaurus) (1.10.0+cu111)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.7/dist-packages (from allosaurus) (0.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allosaurus) (1.4.1)\n",
            "Requirement already satisfied: munkres in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (1.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (3.13)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (2019.12.20)\n",
            "Requirement already satisfied: unicodecsv in /usr/local/lib/python3.7/dist-packages (from panphon->allosaurus) (0.14.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy->allosaurus) (1.15.0)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy->allosaurus) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy->allosaurus) (0.34.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->allosaurus) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install allosaurus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-k-7oiYQ4Ivx"
      },
      "outputs": [],
      "source": [
        "#These libraries help to interact with the operating system and the runtime environment respectively\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "#Model/Training related libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "#Dataloader libraries\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Transforms and datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Allosaurus\n",
        "from allosaurus.audio import read_audio\n",
        "from allosaurus.app import read_recognizer\n",
        "from allosaurus.am.utils import *\n",
        "\n",
        "\n",
        "# Cross validation\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "scRMPhAs_9ig"
      },
      "outputs": [],
      "source": [
        "recognizer = read_recognizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7__m5ma3Qzf",
        "outputId": "06f80da4-3632-42f2-fcf0-8af7ac46a138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10039, 7)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/gdrive/MyDrive/iemocap_full_dataset.csv\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "f1M2EUSJAD3v"
      },
      "outputs": [],
      "source": [
        "df = df[df.emotion != 'xxx']  # only keep data that has emotion label\n",
        "# only keep 'neu', 'hap', 'sad', 'ang' labels\n",
        "df = df.drop(df[~ ((df.emotion == 'neu') | (df.emotion == 'hap') | (df.emotion == 'sad') | (df.emotion == 'ang'))].index)\n",
        "\n",
        "df_unedit = df.copy()\n",
        "df_unedit[\"path\"] = df_unedit[\"path\"].apply(lambda x : x.split('/')[-1])\n",
        "all_files = list(df_unedit.path)\n",
        "file_to_emotion = dict(zip(df_unedit.path, df_unedit.emotion))\n",
        "\n",
        "all_full_files = list(df.path)\n",
        "# print(df)\n",
        "# print(df_unedit)\n",
        "# print(len(file_to_emotion))\n",
        "# print(file_to_emotion)\n",
        "# print(all_full_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAPXqH0zAKHk",
        "outputId": "3a63b6d6-436b-415e-8b0c-e851772377cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neu': 0, 'hap': 1, 'sad': 2, 'ang': 3}\n",
            "{0: 'neu', 1: 'hap', 2: 'sad', 3: 'ang'}\n",
            "Counter({'neu': 1708, 'ang': 1103, 'sad': 1084, 'hap': 595})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# get unique emotions\n",
        "# emotion_to_label = {'neu': 0, 'fru': 1, 'sad': 2, 'sur': 3, 'ang': 4, 'hap': 5, 'exc': 6, 'fea': 7, 'dis': 8, 'oth': 9}\n",
        "emotion_to_label = {'neu': 0, 'hap': 1, 'sad': 2, 'ang': 3}\n",
        "label_to_emotion = {v: k for k, v in emotion_to_label.items()}\n",
        "print(emotion_to_label)\n",
        "print(label_to_emotion)\n",
        "\n",
        "# counter number of class instances\n",
        "emotion_instances_list = [v for v in file_to_emotion.values()]\n",
        "counter = Counter(emotion_instances_list)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_to_label = {k: emotion_to_label[v] for k, v in file_to_emotion.items()}\n",
        "print(file_to_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "c3di3Mr5rRG4"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, file_list, target_list):\n",
        "        \n",
        "        self.file_list = file_list\n",
        "        self.target_list = target_list\n",
        "        self.num_classes = len(list(set(target_list)))\n",
        "\n",
        "        self.x = file_list\n",
        "        self.y = target_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filepath = self.file_list[index]\n",
        "        x = torch.tensor(recognizer.pm.compute(read_audio(filepath)))\n",
        "        x = x.detach()\n",
        "        x_len = torch.tensor(np.array([x.shape[0]], dtype=np.int32))\n",
        "        x_len = x_len.detach()\n",
        "        y = torch.Tensor([self.target_list[index]])\n",
        "        return x, x_len, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tar -xvf  \"/content/gdrive/MyDrive/IEMOCAP_full_release_withoutVideos.tar.gz\" -C \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AK8jRp6isDN2"
      },
      "outputs": [],
      "source": [
        "# collate function\n",
        "def pad_collate(batch):\n",
        "\n",
        "    # batch looks like [(x0, xlen0, y0), (x4, xlen4, y4), (x2, xlen2, y2)... ]\n",
        "    feats = [sample[0] for sample in batch]\n",
        "    feat_lens = [sample[1] for sample in batch]\n",
        "    target_list = torch.Tensor([sample[2] for sample in batch])\n",
        "\n",
        "    feats = pad_sequence(feats, batch_first=True, padding_value=0) # batch, features, len\n",
        "    feat_lens = pad_sequence(feat_lens, batch_first=True, padding_value=0).squeeze()\n",
        "    idx = torch.argsort(feat_lens, descending=True) # sorting the input in descending order as required by the lstms in AM.\n",
        "\n",
        "    targets = target_list[idx]\n",
        "    tensor_batch_feat, tensor_batch_feat_len = move_to_tensor([feats[idx], feat_lens[idx]], device_id=-1) # converting to the required tensors\n",
        "\n",
        "    # Features\n",
        "    output_tensor, input_lengths = recognizer.am(tensor_batch_feat, tensor_batch_feat_len, return_lstm=True)# output_shape: [len,batch,features]\n",
        "    output_tensor = output_tensor.permute(1,2,0)\n",
        "    output_tensor = output_tensor.detach()\n",
        "    input_lengths = input_lengths.detach()\n",
        "    \n",
        "    return output_tensor, input_lengths, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "jwpZTdgHsBjN"
      },
      "outputs": [],
      "source": [
        "all_file_paths = [os.path.join(\"/content\", \"data\", \"IEMOCAP_full_release\", file_path) for file_path in all_full_files]\n",
        "total_instances = len(all_file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abbkoaDuscWe",
        "outputId": "911b93e0-9e0c-4eda-b4ef-4503f9e507ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number training instances: 3592\n",
            "number validation instances: 449\n",
            "number test instances: 449\n"
          ]
        }
      ],
      "source": [
        "num_train = round(0.8 * total_instances)\n",
        "num_test_all = total_instances - num_train\n",
        "num_val = round(0.5 * num_test_all)\n",
        "num_test = num_test_all - num_val\n",
        "\n",
        "print(\"number training instances:\", str(num_train))\n",
        "print(\"number validation instances:\", str(num_val))\n",
        "print(\"number test instances:\", str(num_test))\n",
        "assert(num_train + num_val + num_test == total_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Xw3Mh8bpsYc9"
      },
      "outputs": [],
      "source": [
        "# shuffle data\n",
        "import random\n",
        "random.seed(2021)\n",
        "\n",
        "shuffled_data_paths = random.sample(all_file_paths, k=total_instances)\n",
        "train_list_paths = shuffled_data_paths[:num_train]\n",
        "testall_list_paths = shuffled_data_paths[num_train:]\n",
        "val_list_paths = testall_list_paths[:num_val]\n",
        "test_list_paths = testall_list_paths[num_test:]\n",
        "\n",
        "assert(len(train_list_paths) + len(val_list_paths) + len(test_list_paths) == total_instances)\n",
        "\n",
        "# # train, val, test variables:\n",
        "# train_list_paths\n",
        "# val_list_paths\n",
        "# test_list_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "x6L5WOufrx7i"
      },
      "outputs": [],
      "source": [
        "# get corresponding labels for data\n",
        "train_list_labels = [file_to_label[filepath.split('/')[-1]] for filepath in train_list_paths]\n",
        "val_list_labels = [file_to_label[filepath.split('/')[-1]] for filepath in val_list_paths]\n",
        "test_list_labels = [file_to_label[filepath.split('/')[-1]] for filepath in test_list_paths]\n",
        "\n",
        "assert(len(train_list_labels) == len(train_list_paths))\n",
        "assert(len(val_list_labels) == len(val_list_paths))\n",
        "assert(len(test_list_labels) == len(test_list_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "d_V9_bpjrxg8"
      },
      "outputs": [],
      "source": [
        "# train dataloader\n",
        "train_dset = MyDataset(train_list_paths, train_list_labels)\n",
        "train_args = dict(shuffle=True, batch_size=64, num_workers=4, collate_fn=pad_collate, drop_last=True)  # change to num_workers=4 on diff platform\n",
        "train_loader = DataLoader(train_dset, **train_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ktxCl_ZftaMl"
      },
      "outputs": [],
      "source": [
        "# val dataloader\n",
        "val_dset = MyDataset(val_list_paths, val_list_labels)\n",
        "val_args = dict(shuffle=False, batch_size=64, num_workers=4, collate_fn=pad_collate, drop_last=True)\n",
        "val_loader = DataLoader(val_dset, **val_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Xh_xwODor_O9"
      },
      "outputs": [],
      "source": [
        "def get_k_folder(k, i):\n",
        "  n = num_train+num_val\n",
        "  train_val_list_paths = shuffled_data_paths[:n]\n",
        "  fold_size = n // k\n",
        "\n",
        "  train_list_paths.append(train_val_list_paths[i*fold_size+fold_size:n])\n",
        "  val_list_paths.append(train_val_list_paths[i*fold_size:min(i*fold_size+fold_size, n)])\n",
        "\n",
        "  return train_list_paths, val_list_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ts2DKMHRymHL"
      },
      "outputs": [],
      "source": [
        "#shuffled paths\n",
        "\n",
        "complete_paths=train_list_paths+val_list_paths\n",
        "complete_labels= train_list_labels+val_list_labels\n",
        "complete_dataset=MyDataset(complete_paths,complete_labels)\n",
        "\n",
        "\n",
        "complete_paths_lst=([i for i in complete_paths])\n",
        "complete_labels_lst=([i for i in complete_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "9qnmbsoWYLKY"
      },
      "outputs": [],
      "source": [
        "def kfoldSKLearn(kValue, currentFold, complete_dataset):\n",
        "  # testing k-fold function\n",
        "  k_folds = kValue\n",
        "  # Define the K-fold Cross Validator\n",
        "  kfold = KFold(n_splits=k_folds, shuffle=False)\n",
        "  train_paths=[]\n",
        "  val_paths=[]\n",
        "  train_labels=[]\n",
        "  val_labels=[]\n",
        "  # print(\"K FOLD FUNCTION ACCESSED\")\n",
        "  for fold, (train_ids, val_ids) in enumerate(kfold.split(complete_dataset)):\n",
        "      \n",
        "      train_paths.append([complete_paths_lst[i] for i in train_ids ])\n",
        "      val_paths.append([complete_paths_lst[i] for i in val_ids ])\n",
        "      \n",
        "      train_labels.append([complete_labels_lst[i] for i in train_ids ])\n",
        "      val_labels.append([complete_labels_lst[i] for i in val_ids ])\n",
        "  # print(val_paths[currentFold])\n",
        "  # print(train_paths[currentFold])\n",
        "  train_dset = MyDataset(train_paths[currentFold], train_labels[currentFold])\n",
        "  train_args = dict(shuffle=True, batch_size=64, num_workers=2, collate_fn=pad_collate, drop_last=True)  # change to num_workers=4 on diff platform\n",
        "  train_loader = DataLoader(train_dset, **train_args)\n",
        "\n",
        "  val_dset = MyDataset(val_paths[currentFold], val_labels[currentFold])\n",
        "  val_args = dict(shuffle=False, batch_size=64, num_workers=2, collate_fn=pad_collate, drop_last=True)\n",
        "  val_loader = DataLoader(val_dset, **val_args)\n",
        "\n",
        "  return train_loader,val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "h8ZIiO_l6-i4"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, in_channels =640, out_channels = 256, layers=4, label_size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        kernel = [3,5,7,9]\n",
        "        dil = [1,2,3,4]\n",
        "        pad = []\n",
        "        for i in range(4):\n",
        "          out = int(kernel[i]/2) * (dil[i])\n",
        "          pad.append(out)\n",
        "        \n",
        "        if layers >=1:\n",
        "          self.layer1 = nn.Sequential(\n",
        "                            nn.Conv1d(in_channels, out_channels, kernel_size=int(kernel[0]), stride=1, padding=int(pad[0]), dilation=int(dil[0]), bias=False),\n",
        "                            nn.BatchNorm1d(out_channels),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(p=0.2))\n",
        "    \n",
        "\n",
        "        if layers >=2:\n",
        "          self.layer2 = nn.Sequential(\n",
        "                            nn.Conv1d(out_channels, out_channels, kernel_size=kernel[1], stride=1, padding=pad[1], dilation=dil[1], bias=False),\n",
        "                            nn.BatchNorm1d(out_channels),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(p=0.4))\n",
        "\n",
        "        if layers >=3:\n",
        "          self.layer3 = nn.Sequential(\n",
        "                            nn.Conv1d(out_channels, out_channels, kernel_size=kernel[2], stride=1, padding=pad[2], dilation=dil[2], bias=False),\n",
        "                            nn.BatchNorm1d(out_channels),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(p=0.4))\n",
        "\n",
        "        if layers >=4:\n",
        "          self.layer4 = nn.Sequential(\n",
        "                            nn.Conv1d(out_channels, out_channels // 4, kernel_size=kernel[3], stride=1, padding=pad[3], dilation=dil[3], bias=False),\n",
        "                            nn.BatchNorm1d(out_channels // 4),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(p=0.4))\n",
        "            \n",
        "        self.avg_pool  = nn.Sequential(nn.AdaptiveAvgPool1d(4))\n",
        "\n",
        "        self.last = nn.Sequential(nn.Dropout(p=0.5),nn.Flatten())\n",
        "\n",
        "        if layers<=3: \n",
        "          self.linear = nn.Linear(in_features = out_channels * 4, out_features = label_size)\n",
        "        else: \n",
        "          self.linear = nn.Linear(in_features = out_channels//4 * 4, out_features = label_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, lengths):\n",
        "      \n",
        "      out = self.layer1(input)\n",
        "      if self.layers >=2:\n",
        "        out = self.layer2(out)\n",
        "      if self.layers >=3:\n",
        "        out = self.layer3(out)\n",
        "      if self.layers >=4:\n",
        "        out = self.layer4(out)\n",
        "\n",
        "      out = self.avg_pool(out)\n",
        "\n",
        "      out = self.last(out)\n",
        "\n",
        "      logits = self.linear(out)\n",
        "      return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAxmuMs8jAa"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "eMCs_r1t8g5O"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader, model, opt, criterion, device):\n",
        "\n",
        "    loss_accum = 0.0\n",
        "    batch_cnt = 0\n",
        "\n",
        "    acc_cnt = 0     #count correct predictions\n",
        "    err_cnt = 0     #count incorrect predictions\n",
        "\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
        "        x = x.to(device)\n",
        "        #lengths = lengths.to(device)\n",
        "        y = y.long().to(device)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # print(x.shape)\n",
        "        # print(y.shape)\n",
        "\n",
        "        logits = model(x, lengths)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss_score = loss.cpu().item()\n",
        "\n",
        "        loss_accum += loss_score\n",
        "        batch_cnt += 1\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        #model outputs\n",
        "        out_val, out_indices = torch.max(logits, dim=1)\n",
        "        tar_indices = y\n",
        "\n",
        "        for i in range(len(out_indices)):\n",
        "            if out_indices[i] == tar_indices[i]:\n",
        "                acc_cnt += 1\n",
        "            else:\n",
        "                err_cnt += 1\n",
        "                     \n",
        "    training_accuracy =  acc_cnt/(err_cnt+acc_cnt) \n",
        "    training_loss = loss_accum / batch_cnt\n",
        "        \n",
        "    return model, training_accuracy, training_loss\n",
        "\n",
        "\n",
        "def test_model(loader, model, opt, criterion, device):\n",
        "    model.eval()\n",
        "    acc_cnt = 0\n",
        "    err_cnt = 0\n",
        "\n",
        "    for x, lengths, y in loader:\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.long().to(device)\n",
        "        \n",
        "        logits = model(x, lengths)\n",
        "\n",
        "        out_val, out_indices = torch.max(logits, dim=1)\n",
        "        tar_indices = y\n",
        "\n",
        "        for i in range(len(out_indices)):\n",
        "            if out_indices[i] == tar_indices[i]:\n",
        "                acc_cnt += 1\n",
        "            else:\n",
        "                err_cnt += 1\n",
        "\n",
        "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
        "    \n",
        "    return current_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAOlPe9g80N_"
      },
      "source": [
        "## Main runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "oXwYXAdJ2DLN"
      },
      "outputs": [],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "# print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "dJCPGtZywCmc"
      },
      "outputs": [],
      "source": [
        "file1 = open(\"/content/gdrive/MyDrive/projectlog.txt\",\"a+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TdP3rmfGl9Ll"
      },
      "outputs": [],
      "source": [
        "def reset_weights(m):\n",
        "    if isinstance(m, (nn.Conv1d, nn.Linear, nn.BatchNorm1d)):\n",
        "        m.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Eo52KGm2l-Bv"
      },
      "outputs": [],
      "source": [
        "# model = CNNModel(640, 256, 2, label_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "9D-9YEDro5Ak"
      },
      "outputs": [],
      "source": [
        "# model.apply(reset_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyMP1HFp8y6C",
        "outputId": "23871704-45d0-4e78-e1e8-173269cf773c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNModel(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv1d(640, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,), bias=False)\n",
            "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.4, inplace=False)\n",
            "  )\n",
            "  (avg_pool): Sequential(\n",
            "    (0): AdaptiveAvgPool1d(output_size=4)\n",
            "  )\n",
            "  (last): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (linear): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            ".........Running 0th cross validation.......\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fddcb43e710>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fddcb43e710>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fddcb43e710>\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "Traceback (most recent call last):\n",
            "AssertionError: can only test a child process\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fddcb43e710>\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "AssertionError: can only test a child process\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "n_epochs = 10\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "#Define Training Grid Search\n",
        "in_channels = [640]\n",
        "out_channels = [64, 128, 256]\n",
        "layers = [2, 3, 4]\n",
        "\n",
        "for layer in layers:\n",
        "    for in_channel in in_channels:\n",
        "        for out_channel in out_channels:\n",
        "\n",
        "            model = CNNModel(in_channel, out_channel, layer, label_size=4)        \n",
        "\n",
        "            device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "            model.to(device)\n",
        "            \n",
        "            print(model)\n",
        "\n",
        "            k = 5\n",
        "            \n",
        "            avg_val_acc = 0\n",
        "  \n",
        "            for i in range(k):\n",
        "\n",
        "              print(f'.........Running {i}th cross validation.......')\n",
        "\n",
        "              ## Reset weights for each fold\n",
        "\n",
        "              model.apply(reset_weights)\n",
        "\n",
        "              opt = optim.Adam(model.parameters(), lr = 0.001, weight_decay=1e-6)\n",
        "              criterion = nn.CrossEntropyLoss()\n",
        "              scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, patience=2)\n",
        "\n",
        "              ## To reload saved model\n",
        "\n",
        "              # key = str(i) + '-' + str(layer) + '-' + str(in_channel) + '-' + str(out_channel)\n",
        "              # path = '/content/gdrive/MyDrive/model/{i}.pt'.format(i=key)\n",
        "              # checkpoint = torch.load(path)\n",
        "              # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "              # opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "              # saved_epoch = 15\n",
        "\n",
        "              for n in range(0, n_epochs):      \n",
        "                  \n",
        "                  train_loader, val_loader =  kfoldSKLearn(k, i,complete_dataset)\n",
        "\n",
        "                  model, train_acc, train_loss = train_model(train_loader, model, opt, criterion, device)\n",
        "\n",
        "                  valid_acc = test_model(val_loader, model, opt, criterion, device)\n",
        "    \n",
        "                  scheduler.step(valid_acc)\n",
        "            \n",
        "                  print(\"Epoch: \"+str(n)+ \", Fold: \" + str(i) + \", Training Accuracy: \" +str(train_acc)+ \", Training loss:\"+str(train_loss)+ \", Validation accuracy:\" +str(valid_acc))\n",
        "\n",
        "                  #Logging the results of the 10th epoch \n",
        "\n",
        "                  key = str(n) + '-' + str(i) + '-' + str(layer) + '-' + str(in_channel) + '-' + str(out_channel) + '-' + str(valid_acc) + '\\n'\n",
        "                  \n",
        "                  file1.write(key)\n",
        "              \n",
        "              # Considering the validation acc of the last epoch for each of the k folds\n",
        "\n",
        "              avg_val_acc+=valid_acc\n",
        "\n",
        "              p = str(i) + '-' + str(layer) + '-' + str(in_channel) + '-' + str(out_channel)\n",
        "\n",
        "              path = '/content/gdrive/MyDrive/model/{i}.pt'.format(i=p)\n",
        "\n",
        "              torch.save({\n",
        "                      'model_state_dict': model.state_dict(),\n",
        "                      'optimizer_state_dict': opt.state_dict(),\n",
        "                      'scheduler_state_dict' : scheduler.state_dict(),\n",
        "                      }, path) \n",
        "\n",
        "            avg_val_acc/=k\n",
        "            print(\"Average Val Accuracy: \" + str(avg_val_acc))  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLNCyppJ5Jpx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "CNNEmbed.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
